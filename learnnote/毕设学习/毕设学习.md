cnn：卷积神经网络 常用于图像识别或和图像相关的领域中
rnn：循环神经网络 用于语义识别 语音识别 和文本识别  文本处理
lstm：长短时记忆网络 用于语境分析 多人对话

几个概念的对比：

![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/%E5%87%A0%E4%B8%AA%E6%A6%82%E5%BF%B5%E5%AF%B9%E6%AF%94.png)
**在机器学习中，手动选择用于对图像进行分类的特征和分类器。在深度学习中，特征提取和建模步骤是自动化的。即深度学习自动的找出这个分类问题所需要的重要特征，而传统机器学习则需要我们人工的给出特征**


MNIST：手写数字的数据库
Flask：使用Python语言编写的轻量级web应用框架








###大规模神经网络有两个通病
1、费时
2、过拟合
而每次做完dropout相当于从原始网络中找到了更瘦的网络，并且对于有n个节点的网络来说，相当于有2的n次方个模型，但是训练的参数数目确是不变的，这就解决了费时的问题





###adaboost算法核心
针对同一个训练集训练不同的分类器，弱分类器多次迭代，每次迭代后会进行反馈，让下一轮迭代更注重上次迭代无法分类的样本

###cnn训练算法包含两个方向
####前向传播
从输入层输入图像样本，然后通过卷积层进行卷积操作，通过局部感知野和权值共享额方法来提取初级特征，再传递到池化层进行采样。经过多次卷积采样向下传递到输出层。
####反向传播
由于前向传播得到的输出值和理想输出值误差较大，通过反向传播使用梯度下降算法来减小误差。

###cnn的主要优点
**第一采用局部感知野**，根据图像空间的联系是局部的这一特点，令每一个神经元只感受到局部的图像区域，而非感受全局图像。**第二采用权值共享**，根据神经元之间是局部连接的这一特性，令每一个神经元采用相同的卷积核对图像做卷积，再根据不同的的图像特征采取不同的卷积核。
对输入的大小相同的图片进行同样的计算，这样做是因为：对图片等数组数据来说，局部数组的值是高度相关的，可以容易探测到独特的局部特征。第二，图像与其他信号的局部统计特征是高度不相关的，如果特征图能在图片的一个一个部分出现，也能出现在任何地方。所以不同的位置的单元共享同样的权重，并在数组的不同部分探测相同的模式。

###人脸配准
在人脸检测的基础上去除旋转，遮挡等因素的影响，准确定位人脸特征点的方法。
  

###PCA
主成分分析法：利用协方差矩阵来分析元素的相关性，选出图像的主成分，排除其他的冗余信息，形成一个变换矩阵，通过变换矩阵实现正交变换，对原有的高维图像进行降维。	
第一步：选取训练集，第二部特征中心化，计算数据的平均值，用原数据减去平均值，第三步计算元素之间的相关性，按照从大到小的顺序排列，选取前M个特征值计算相应的特征向量，将特征向量组成的变换矩阵P进行线性变换 然后实现降维，利用正交变换实现降维 	P=M*T  X=T*N  Y=M*N M<T 实现了属性的降维

###LBP
局部二值模式：通过计算图像中所包含的每个像素与其邻域8的点的灰度值的关系，形成一个二进制编码，其中中心值为阈值，领域内大于它的就为1 小于的就为0。最后采用多区域直方图作为图像的特征描述，根据计算两个图像lbp直方图的距离来判断两者的相似性。

###其他几种特征提取方法
ASM,AAM,尺度不变特征转换，外貌特征提取方法，Gabor滤波器，



###单通道
俗称灰度图，每个像素点只有一个值表示颜色
###三通道图
每个像素点都有3个值表示，RGB（红，绿，蓝），其他颜色由这三色叠成

###批规范化
batch normalization：在采用梯度下降算法训练nn时，对网络层的每个mini—batch的数据进行归一化，使其均值变为0，方差变为1，作用是缓解dnn在训练中的梯度消失或爆炸现象，加快模型训练速度


###卷积

就是把一个二元函数变成一个一元函数，降维 例如一个毛毯是个二维平面，把它卷起来就可以看成类似一维的函数，此函数的每个点的函数值等于那原来那个平面的一元函数的积分值

![](https://upload-images.jianshu.io/upload_images/7828401-21e58785f002ef83.gif)
**粉红色最后的卷积结果矩阵维度=绿色矩阵维数-橙色矩阵维数+1**

![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/卷积2.png)
**卷积结果作为卷积核中间像素对应的图片上像素的灰度值**
**卷积核在神经网络中代表对应的权重**

###卷积核的选择
+ 卷积核大小一般为奇数，这样它是按照中间的像素点中心对称的，一般为3$\times$3或5$\times$5或7$\times$7，有中心就有半径，例如5$\times$5的核，半径就是2
+ 卷积核所有元素之和一般要为1，这是为了原始图像的能量（亮度）守恒
+ 如果滤波器矩阵之和大于1，则滤波后的图像会比原图要亮，如果小于1，则会比原图暗一些，如果等于0，图像不会变黑，但也会非常暗




###padding

![](https://upload-images.jianshu.io/upload_images/7828401-32f103e8ba676203.gif)

###填充分类
####补零填充
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/补零填充.png)

####边界复制填充
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/边界复制填充.png)

####镜像填充
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/镜像填充.png)

####块填充
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/块填充.png)
**填充的好处**
+ 可以获得更多更细致的特征信息，例如可以获得图像更多的边缘信息，因为不填充可能图像外层一圈都损失掉
+ 控制输出的特征图尺寸，达到控制网络结构的功能
  



###最大池化
用Maxpooling算法来缩减像素的采样数组，按照2*2来分割矩阵，分出每一个网络中只保留最大值数组，丢弃其他数组，得到最大池化数组
接下来最大池化数组作为另一个神经网络的输入，这个全连接神经网络会最终计算出此图是否符合预期。

![池化](https://upload-images.jianshu.io/upload_images/7828401-182a816b5b0633e1.gif "池化图")

###全连接层

conv：   1.相当于一个特征提取器来提取特征
　　　　2.提供了位置信息
　　　　3.减少了参数个数
pooling： 1.提取特征
　　　　   2.减少参数

激活函数：增加网络的非线性表达能力

把特征整合到一起，输出为一个值，大大减少特征位置对分类的影响，作用是分类
例如：
![](https://pic1.zhimg.com/80/v2-de4ba4bac6abed53025026f877fd80d1_hd.jpg)
因为全连接的作用是分类：
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/%E7%8C%AB%E5%85%A8%E8%BF%9E%E6%8E%A5.jpg)
红色的神经元表示这个特征被找到了（激活了），同一层的神经元要么是猫的特征不明显，要么没找到，当把这些特征组合起来，发现最符合要求的是猫
对于输入的每一张图，用了一个和图像一样大小的核卷积，这样整幅图就变成了一个数了，如果厚度是20就是那20个核卷积完了之后相加求和。这样就能把一张图高度浓缩成一个数了。


###人脸图像的预处理
主要是人脸校正，人脸图像的增强（改善人脸图像的质量），以及归一化的操作

###直方图均衡
是一种点操作，逐点改变图像的灰度值，尽量使各个灰度级别都具有相同的数量像素点，使直方图均衡，可使输入图像转换为在每个灰度级上都具有相同的像素点数的输出图像（即输出图像是平的）

###中值滤波
无论获取的图像是灰度图还是彩色图像转变的，里面都有噪声的存在，进行中值滤波不仅可以去除孤点噪声，而且可以保持图像的边缘特性，不会使图像产生显著的模糊

###过拟合（overfit）
简单理解就是训练样本得到的输出和期望输出基本一致，但是测试样本输出和期望输出相差却很大，为了得到一致假设而使得假设变得过度复杂称为过拟合。想象某种算法产生一个过拟合的分类器，这个分类器能百分之百正确分类样本数据，但也就是能够完全正确的分类，使得规则如此严格，以致任何与样本稍有不同的文档它都认为不属于这个类别

###PLS算法（偏最小二乘法）
最小二乘法的缺点在于它引用了x变量的全部数据，无关变量也牵涉其中，对结果造成影响，偏最小二乘法主要使用成分提取的理念，PCA的想法，用指定的成分数量，对数据最大程度提取，不断从残差中提取有效信息，对最终结果进行求解，得到拟合系数。

###步数为何是225？
batch_size=128
训练集的图像数为28821
则每一个echo有28821/128=225步

###relu，sigmoid，softmax
relu(线性整流函数，又称修正线性单元):$φ(x) = max(0,x)$
$sigmod=\cfrac{1}{1+e^{-x}}$,导数会慢慢趋于0，造成梯度消失现象，而relu的导数为0或1，x小于0时为0，x大于0时为1

###bp神经网络
称为前馈神经网络，信号的前向传播，误差反向传播算法。如果输出层不能得到期望的输出，则转向反向传播，将误差信号沿原路返回，修改各神经元的权值，使得误差信号最小。

###dropout
让某个神经元以概率p停止工作，其实就是让它的激活值以概率p变为0，比如我们某一层网络神经元的个数为1000个，其激活值为x1，x2……x1000，我们dropout比率选择0.4，那么这一层神经元经过drop后，x1……x1000神经元其中会有大约400个的值被置为0。

###激活函数的性质
作用就是将输入数据映射到0到1之间（tanh是映射到-1到1之间）
1. 非线性
2. 可微性
3. 单调性
4. 输出值范围

###正则化
L1正则化和L2正则化可以看做损失函数的惩罚项，所谓惩罚就是对损失函数中的某些参数做一些限制。
L1正则化产生稀疏矩阵，L2正则化防止过拟合

###卷积不想降低维度
300$\times$300的原始矩阵，通过3$\times$3的卷积核来扫，扫出来的矩阵按照公式为300-3+1=298，若想扫完仍为300$\times$300，则padding，在原矩阵外层包一层0，即变为302$\times$302的矩阵，按公式扫完后维度不变


###几种激活函数
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/%E6%88%AA%E5%9B%BE.png)

###梯度下降算法
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.png)

###感受野计算方法
以卷积核尺寸k=5,步长s=2为例，计算示例图如下：

![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97.png)

###Conv2d中的padding
padding = valid ：without padding

```
  inputs: 1  2  3  4  5  6  7  8  9  10 11 (12 13)
          |______________|                dropped
                        |_________________|
```
padding = same ：with zero padding
```
            pad|                                      |pad
   inputs:     0 |1  2  3  4  5  6  7  8  9  10 11 12 13|0  0
               |______________|
                              |_______________|
                                             |________________|
```
在这个例子中：
Input width = 13
Filter width = 6
Stride = 5

###keras中的JSON和H5文件
1.model.to_json()
将模型序列化保存为JSON文件，记录网络的整体结构，以及参数的设置
2.model.save_weights()
经过调参后对输出精度比较满意，则把训练好的网络权值参数保存起来


###混淆矩阵
假设训练之初以及预测后，一个样本是正例还是反例是已经确定的，这个时候，样本应该有两个类别值，一个是真实的0/1，一个是预测的0/1
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5.jpg)
TP（实际为正预测为正），FP（实际为负但预测为正），TN（实际为负预测为负），FN（实际为正但预测为负）

**查全率（召回率 recall）**：样本的正例有多少被预测准确了，预测对的正例数占真正的正例数的比率
$$查全率 = \cfrac{TP}{TP+FN}$$

**查准率(精准率，precision)**：针对预测结果而言，预测为正的样本有多少是真正的正样本，衡量的是查准率，预测正确的正例数占预测为正例总量的比率：
$$查准率 = \cfrac{TP}{TP+FP}$$

**准确率**：反映分类器统对整个样本的判定能力，能将正的判定为正，负的判定为负的能力
$$准确率 = \cfrac{TP+TN}{TP+FP+FN+TN}$$

P-R曲线使用查准率，查全率为纵，横坐标。

目标检测中的mAP指标就是PR曲线的面积（求积分）

ROC曲线使用纵轴“真正例率”（TPR），横轴“假正例率”（FPR）

$$真正例率TPR=\cfrac{TP}{TP+FN}$$
$$假正例率FPR=\cfrac{FP}{TN+FP}$$


**阴性预测值**：可以理解为负样本的查准率，阴性预测值被预测准确的比例
$$阴性预测值 = \cfrac{TN}{TN+FN}$$

查准率和查全率通常是一对矛盾的度量，通常一个高，另外一个就低。两个指标都很重要，我们应该如何综合考虑这两个指标呢？
主要有两种办法：
**平衡点**：查准率=查全率的点
**F1度量**：查准率和查全率的加权调和平均数
**(1)**.当认为查准率和查全率同样重要（权重相同）时:
$$F1 = \cfrac{1}{\cfrac{1}{Recall}+\cfrac{1}{Precision}}=\cfrac{2\times Recall \times Precison}{Recall + Precision}$$
**(2)**.当查准率和查全率重要性不同时（权重不同）：
$$F_\beta = \cfrac{(1+\beta ^2)\times P \times R}{\beta ^2 \times P + R}$$
其中：
$$\beta ^2 = \cfrac{R_{weight}}{P_{weight}}$$   

$$R_{weight}+P_{weight} = 1$$
可以推导得：
$$1+ \beta ^2 = \cfrac{1}{P_{weight}}$$
带权重的调和平均数公式如下：
$$\cfrac{1}{F_ \beta} = \cfrac{1}{\cfrac{P_{weight}}{p}+\cfrac{R_{weight}}{R}}$$
化简得$F_ \beta$式


###Adam算法
基于训练数据迭代的更新神经网络的权重，与传统的随机梯度下降不同的是，Adam通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率

###Mobilenet
它使用深度可分离卷积操作，意思是说采用的是单通道卷积操作，而不是混合三种颜色然后进行扁平化操作
**普通卷积**
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/普通卷积.png)
**mobilenet卷积**
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/mobilenet卷积.png)

###为什么使用交叉熵作为损失函数
当损失函数为凸函数时，梯度下降算法才能保证达到全局最优解
如果使用均值方差作为损失函数，其曲线是凹函数

###flatten层
输入压平，将多维的输入一维化，常用于从卷积到全连接层的过渡

###深度可分离卷积
可分为两步：
 + 用三个卷积核分别对三个通道做卷积，这样在一次卷积后则得到3个数，即可看作是一个1\*1*3的向量 
 + 对于输出的三个数，再通过一个1\*1*3的卷积核（pointwise核），得到一个数

所以深度可分离卷积是通过两次卷积实现的
+ 第一步，对三个通道分别做卷积，输出三个通道的属性：
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/深度可分离第一步.png)
+ 第二步，用卷积核1x1x3对三个通道再次做卷积，这个时候的输出就和正常卷积一样，是8x8x1：
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/深度可分离第2步.png)
+ 如果要提取更多的属性，则需要设计更多的1x1x3卷积核心就可以(感觉应该将8x8x256那个立方体绘制成256个8x8x1，因为他们不是一体的，代表了256个属性)

假设 Input 是 28×28×192 的 Feature Maps，在通道相关性上利用 32 个 1×1×192 的卷积核做线性组合，得到 28×28×32 大小的 Feature Maps，再对这些 Feature Maps 做 256 个 3×3×32 的卷积，即联合映射所有维度的相关性，就得到 28×28×256 的 Feature Maps 结果。可以发现，这个结果其实跟直接卷积 256 个3×3×192 大小的卷积核是一样。也就是说，Inception 的假设认为用 32 个 1×1×192 和 256 个 3×3×32 的卷积核退耦级联的效果，与直接用 256个 3×3×192 卷积核等效。而两种方式的参数量则分别为32×1×1×192 + 256×3×3×32 = 79872 和 256×3×3×192 = 442368。


###卷积池化输出大小计算
+ 总结：卷积输出大小=[（输入大小-卷积核（过滤器）大小+2*P）／步长]+1 （p是填充的个数）
+ 总结：池化输出大小=[（输入大小-卷积核（过滤器）大小）／步长]+1

###top-5错误率
imagenet图像通常有1000个可能的类别，对每幅图像你可以猜5次结果(即同时预测5个类别标签)，当其中有任何一次预测对了，结果都算对，当5次全都错了的时候，才算预测错误，这时候的分类错误率就叫top5错误率
top1就是你预测的label取最后概率向量里面最大的那一个作为预测结果，你的预测结果中概率最大的那个类必须是正确类别才算预测正确。而top5就是最后概率向量最大的前五名中出现了正确概率即为预测正确。

###xception
xception是由inception结构，加上深度可分离卷积，再加上残差网络结构改进而来

###神经网络为什么要加偏移量
我们可以把问题先简化，为什么线性模型要加 bias？答案很简单，不加 bias 你的分类线(面)就必须过原点，这显然是不灵活的。有了bias我们就可以上下左右移动我们的线了。神经网络是一样的道理。
