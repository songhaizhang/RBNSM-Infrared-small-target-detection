###机器学习的组成
&emsp; **主要任务**
+ 分类：将实例数据划分到合适的类别中
+ 回归：主要用于预测数值型数据

###当使用训练的模型来预测未知数据产生较大误差时，可采取以下措施：
1. 尝试减少特征数量
2. 尝试获得更多特征
3. 尝试增加多项式特征
4. 尝试减少正则化程度$\lambda$
5. 尝试增加正则化程度$\lambda$

###knn算法
k近邻思想：
&emsp; 给定测试样本，基于某种距离度量找出训练集中与其最靠近的k个训练样本，然后基于者k个邻居的信息来进行预测。
&emsp; 它是懒惰学习的著名代表：此类学习在训练阶段仅仅是把样本保存起来，训练时间开销为零，待收到测试样本后再进行处理；相应的，那些在训练阶段就对样本进行学习处理的方法称为急切学习。


&emsp; 就像如何区分爱情片和动作片
+ 爱情片：亲吻次数更多
+ 动作片：打斗的次数更多

&emsp; 根据打斗的次数和接吻的次数，使用k-近邻算法，就可以自动划分电影的题材。
&emsp; 即分类时，对新的实例，根据其最近邻的k个训练实例，通过多数表决等方式进行预测。
&emsp; **三要素：**
&emsp; k值的选择，距离度量以及分类决策规则
&emsp; **knn工作原理**
1. 假设有一个带标签的样本数据集，包含数据和分类的所属关系
2. 输入无标签的数据后，将新数据的每个特征与样本集中的数据的对应特征进行比较
   1. 计算新数据与样本数据集每条数据之间的距离
   2. 对所求得的所有距离进行排序（从小到大，越小即越相似）
   3. 取前k（一般小于等于20）个数据的标签
3. 求k个数据中出现次数最多的标签作为新数据的分类

###几种距离
&emsp; **1. 欧式距离:**
&emsp; 类似空间两点间的距离公式。例子：计算向量(0,0)、(1,0)、(0,2)两两间的欧式距离。D= 1.0000   2.0000    2.2361($\sqrt{\smash[b]{5}}$)
&emsp; **2. 曼哈顿距离（D~4~距离）:**
&emsp; 想象两十字路口之间距离，不是直线最短，因为你无法穿越大楼。(两点投影到各轴上的距离总和)所以也称**城市街区距离（CityBlock distance）**。例子：计算向量(0,0)、(1,0)、(0,2)两两间的曼哈顿距离。D=    1    2     3
&emsp; **3. 汉明距离:**
&emsp; 两个等长字符串之间的汉明距离是两个字符串对应位置的不同字符的个数
&emsp; **4. 切比雪夫距离:（D~8~距离）**
&emsp; 两点投影到各轴上距离的最大值
&emsp; **5. 闵可夫斯基距离:**
&emsp; $(\sum_{i=1}^n|x_i-y_i|^p)^{1/p}$
&emsp;p=1时为曼哈顿距离，2为欧式距离，趋于无穷为切比雪夫距离。

###偏差，误差，方差
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/%E6%96%B9%E5%B7%AE%E5%81%8F%E5%B7%AE.jpg)

###二范数
**矩阵二范数：** 矩阵A的2范数就是A矩阵的转置乘以A矩阵的特征根的最大值的开根号
**向量二范数：** 向量x的2范数就是x中的各个元素平方之和再开根号
**函数二范数：** 函数f(x)的2范数就是x在区间(a,b)上f(x)的平方的积分再开根号

**向量范数**
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/%E8%8C%83%E6%95%B0.jpg)

**矩阵的范数：**
ord=1：列和的最大值

ord=2：|λE-ATA|=0，求特征值，然后求最大特征值得算术平方根

ord=∞：行和的最大值

ord=None：默认情况下，是求整体的矩阵元素平方和，再开根号。（没仔细看，以为默认情况下就是矩阵的二范数，修正一下，默认情况下是求整个矩阵元素平方和再开根号）

**axis：处理类型**

axis=1表示按行向量处理，求多个行向量的范数

axis=0表示按列向量处理，求多个列向量的范数

axis=None表示矩阵范数。

**keepding：是否保持矩阵的二维特性**

True表示保持矩阵的二维特性，False相反

###矩阵的共轭
共轭矩阵相当于把矩阵中的复数的虚部前面的符号变成相反号

###关于axis 1行0列

###超参数
&emsp; 在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。

###贝叶斯定理
$$P(X|Y) = \cfrac{P(X,Y)}{P(Y)} $$
$$=\cfrac{P(Y|X)P(X)}{P(Y)}$$
$$p(类别|特征)=\cfrac{p(特征|类别)p(类别)}{p(特征)}$$
&emsp; 其中**P(X)**:先验概率，表示每种类别分布的概率，**P(Y|X)** 表示类条件概率，表示在某种类别前提下，某事发生的概率，而**P(X|Y)** 表示后验概率，表示某事发生了，并且它属于某一类别的概率，有了这个后验概率，我们就可以对样本进行分类。后验概率越大，说明某事物属于这个类别的可能性越大，我们越有理由把它归到这个类别下。
&emsp; 由此我们只需要对先验概率和类条件概率进行估计，就可以得出后验概率。先验概率估计较简单，但是类条件概率估计非常难，因此把对类条件概率的估计，转换成对参数的估计，**极大似然估计**就是一种参数估计方法。
####极大似然估计（MLE）
就是已知样本结果信息，反推最具有可能的导致这些样本结果的出现的模型参数值。提供了一种给定观察数据来评估模型参数的方法，即“模型已定，参数未知”
极大似然需满足所有采样都是独立同分布假设。
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%8E%9F%E7%90%86.jpg)
抽样分布最接近假设函数求出的对抽样的分布就是最大似然估计

####概率和统计的区别
+ **概率**研究的问题是，已知一个模型和参数，怎么去预测这个模型产生的结果的特性（例如均值，方差，协方差等等）。 举个例子，我想研究怎么养猪（模型是猪），我选好了想养的品种、喂养方式、猪棚的设计等等（选择参数），我想知道我养出来的猪大概能有多肥，肉质怎么样（预测结果）。

+ **统计**研究的问题则相反。统计是，有一堆数据，要利用这堆数据去预测模型和参数。仍以猪为例。现在我买到了一堆肉，通过观察和判断，我确定这是猪肉（这就确定了模型。在实际研究中，也是通过观察数据推测模型是／像高斯分布的、指数分布的、拉普拉斯分布的等等），然后，可以进一步研究，判定这猪的品种、这是圈养猪还是跑山猪还是网易猪，等等（推测模型参数）。

**即：概率是已知模型和参数，推数据，统计是已知数据推模型和参数。**

对于$P(x|\theta)$,
输入有两个：x表示某一具体数据，$\theta$表示模型参数
+ 如果$\theta$确定，x未知，则称它为概率函数，它描述对于不同的样本点x，其出现的概率是多少
+ 如果$\theta$未知，x确定，则称它为似然函数，它描述对于不同的模型参数，出现x这个样本点的概率是多少

####最大后验概率估计（MAP）
最大似然估计是求参数，使得似然函数$P(x_0|\theta)$最大。而最大后验概率估计则是想求参数$\theta$使得$P(x_0|\theta)P(\theta)$最大，求得的参数不仅仅让似然函数大，它自己的先验概率也要大。

####朴素贝叶斯定理
**”朴素“代表着特征间相互独立。**
例如：
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/嫁.jpg)
现在给出问题，如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？
分析可知，求得就是p(嫁|(不帅、性格不好、身高矮、不上进))与p(不嫁|(不帅、性格不好、身高矮、不上进))的概率，谁的概率大，我就能给出嫁或者不嫁的答案！
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/嫁1.jpg)
然后根据表格，根据中心极限定理可知，在样本数量足够大的时候，频率等于概率，故可以求出公式中各值。
####拉普拉斯修正
由于是联乘，所以如果只要有一个概率为0，就会导致整体为0，这样就抹去了其它信息。因此需要平滑：
$$\hat{P}(c)=\cfrac{|D_c|+1}{|D|+N}$$
$$\hat{P}(x_i|c)=\cfrac{|D_{c,x_i}|+1}{|D_c|+N_i}$$
其中，N代表训练集D中可能的类别数，N~i~表示第i个属性的可能取值。
####半朴素贝叶斯分类器
朴素贝叶斯分类器建立在属性条件独立性假设的情况下，但这在现实条件下往往很难成立。因此尝试对属性条件独立性假设进行一定程度的放松。
**独依赖估计（ODE）：** 假设每个属性在类别之外最多仅依赖于一个其它属性。此时的条件概率可以转换为：
$P(x|c)=P(x1|c,pa1)\times P(x2|c,pa2)\times P(x3|c,pa3)\times ... \times P(xd|c,pad)$
其中pai为属性xi所依赖的属性，称为父属性（发出箭头的属性），不同的做法产生不同的独依赖分类器。**c是label标签，x是样本。p(c|x)代表拿到一个样本，这个样本的属性为x1，x2...，在这种情况下，求得样本分类为c的概率。**
1. 所有属性都依赖于同一个属性（SPODE）
2. 最大带权生成树算法基础上构建依赖（TAN）
   1. 计算两个属性之间的条件互信息，
   ![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/条件互信息.jpg)
   刻画了属性x~i~与x~j~在已知类别的情况下的相关性
   1. 以属性为节点构建完全图，任意两个结点之间之间边的权值是I(x~i~,x~j~|y)
   2. 构建此完全图的最大带权生成树，挑选根变量，将边置为有向
   ![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/tan1.jpg.png)
   1. 加入类别节点y，增加从y到每个属性的有向边。
   ![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/tan2.jpg.png)
   对于上图，c即时y
3. 对每个属性构建SPODE，并将结果集成起来作为最终结果（AODE），尝试将每个属性作为超父属性来构建SPODE
####贝叶斯网络（信念网，有向图模型）
**无向图模型 称为马尔科夫随机场**
频率派：参数$\theta$是固定的，而样本X时随机的
贝叶斯派：参数是随机的，样本是固定的。
**联合概率：** 表示两个事件同时发生的概率。P(A$\cap$B)
**边缘概率（先验概率）：** 是某个事件发生的概率。
如下图贝叶斯网络图：
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/贝叶斯网络图.jpg)
上图的联合概率为P(x1,x2,...,x7)=
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/联合密度分布.jpg)
从上图可以得出几个结论：
1. x1,x2,...,x7的联合分布为上式
D-separation的三种情况：
2. x4未知情况系，x1和x2独立（head to head）
3. x6和x7在x4给定的条件下独立（tail to tail）
4. 在head to tail（即链式的情况下），如a->c->b，在c未知时，a，b不独立，在c已知时，a，b独立。**即在xi给定的条件下，xi+1的分布，和x1，x2，...，xi-1条件独立。通俗来讲，当前状态只跟上一状态有关，这种顺次演变的随机过程，叫做马尔科夫链**
    &emsp; 从结点推广到结点集，对于a，b，c三个结点集，若要a，b独立，则需要所有路径都阻断。即满足以下两个条件：
   1. A和B的“head-to-tail型”和“tail-to-tail型”路径都通过C；
   2. A和B的“head-to-head型”路径不通过C以及C的子孙；
如下图，D-separation的三种情况：
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/Dsep.jpg)

由贝叶斯网络构造因子图：
+ 贝叶斯网络中的一个因子对应因子图中的一个结点
+ 贝叶斯网络中的每个变量在因子图上对应边或者半边
+ 结点g与边x相连当且仅当变量x出现在因子g中。

####EM算法(期望最大化算法)
是一种从不完全数据或有数据丢失的数据集（存在隐含变量）中求解概率模型参数的最大似然估计方法。
详情参考:<a href="https://www.jianshu.com/p/1121509ac1dc">EM算法详解</a> 或 <a href="https://blog.csdn.net/v_july_v/article/details/81708386">EM算法JULY</a>

简单例子：
如大厨想往两个盘子里盛菜，那么如何将两个盘子里的菜变得一样大小？可以先将菜按自己的感觉放到两个盘子中，然后再将两个盘子的菜来回均匀，重复多次，最后大致一致。
在这个例子中，**平均分配这个结果就是“观测数据z”，为实现平均分配而每个盘子里分配多少菜就是“待求参数”，分配菜的手感就是“概率分布”**。

**EM算法的主要思想：**
1. 给$\theta$自主规定一个初值（既然我不知道想实现“两个碟子平均分配锅里的菜”的话每个碟子需要有多少菜，那我就先估计个值）
2. 根据给定观测数据和规定的参数$\theta$,求为观测数据z的条件概率分布期望。（在上一步中，已经根据手感将菜倒进了两个碟子，然后这一步根据“两个碟子里都有菜”和“当前两个碟子都有多少菜”来判断自己倒菜的手感）；
3. 由于上一步的z已经求出来了，于是根据极大似然估计求最优的参数${\theta}$^'^（手感已经有了，那就根据手感判断下盘子里应该有多少菜，然后把菜匀匀）；
4. 因为第二步和第三步的结果可能不是最优的，所以重复第二步，第三步，直到收敛。（重复多次匀匀的过程，直到两个碟子中菜的量大致一样）。
上面第二步称作E步（求期望）,第三步被称为M步（）

####Jensen不等式
+ 如果f是凸函数，X是随机变量，则E[f(X)]>=f(E[X]),当f是严格的凸函数时，E[f(X)]>f(E[X])
+ 如果f是凹函数，X是随机变量，则E[f(X)]<=f(E[X]),当f是严格的凹函数时，E[f(X)]<f(E[X])

![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/jensen.jpg)

###Hessian矩阵
对于f(x1,x2,...,xn),如果f的所有二阶导数都存在，则海森矩阵就是对x（第一行：x1x1,x1x2,x1x3,...,x1xn）分别求二阶偏导，由这些二阶偏导组成的矩阵叫海森矩阵。

###正定矩阵与半正定矩阵
1. 给定一个大小为 n*n的实对称矩阵A ，若对于任意长度为n的向量x，有 xTAx>0恒成立，则矩阵A是一个正定矩阵
2. 给定一个大小为 n*n的实对称矩阵A ，若对于任意长度为n的向量x，有 xTAx>=0恒成立，则矩阵A是一个半正定矩阵


###分类决策树
####定义：
&emsp; 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。
####决策树须知：
1. **熵：** 熵指的是体系的混乱的程度
2. **信息熵：** 首先理解信息，**信息量的度量就等于不确定性的多少**
    寻找一个函数I(x)，它是概率p(x)的单调函数，表达了信息的内容。如果我们有两个不相关的事件 x 和 y，那么观察两个事件同时发生时获得的信息量应该等于观察到事件各自发生时获得的信息之和，即：I(x,y)=I(x)+I(y)。
    因为两个事件是独立不相关的，因此 p(x,y)=p(x)p(y)。根据这两个关系，很容易看出 I(x)一定与 p(x) 的对数有关 ：I(x)=−logp(x)
    验证如下：
    $$I(x,y)=-log(p(x,y))$$
    $$=-log(p(x)p(y))$$
    $$=-(logp(x)+log(p(y)))$$
    $$=-(I(x)+I(y))$$
**熵公式：** $$H(X,Y) = -\sum_{x}p(x)logp(x)=-\sum_{i=1}^{n}p(x_i)logp(x_i)$$
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/h(x)%E5%B0%8F%E4%BA%8Elogn(1).png)
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/h(x)%E5%B0%8F%E4%BA%8Elogn(2).png)

1. **相对熵：** 用来衡量两个概率分布之间的差异
2. **条件熵：** 表示在已知随机变量 X 的条件下随机变量 Y 的不确定性。条件熵 H(Y|X) 相当于联合熵 H(X,Y) 减去单独的熵 H(X)，即
H(Y|X)=H(X,Y)−H(X)
5. **交叉熵：** ![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/%E4%BA%A4%E5%8F%89%E7%86%B5.jpg)
   **当用非真实分布 q(x) 得到的平均码长比真实分布 p(x) 得到的平均码长多出的比特数就是相对熵**
1. 希望学到的模型的分布和真实分布一致，P(model)≃P(real)
2. 但是真实分布不可知，假设训练数据是从真实数据中独立同分布采样的，P(train)≃P(real)
3. 因此，我们希望学到的模型分布至少和训练数据的分布一致，P(train)≃P(model)
**交叉熵可以用来计算学习模型分布与训练分布之间的差异**
####决策树学习的三个步骤
1. 特征选择
2. 决策树的生成
3. 决策树的修剪

####信息增益
&emsp; 以某特征划分数据集前后的熵的差值，由于熵表示集合的不确定性，熵越大，表示集合越不稳定。因此可以使用划分前后集合熵的差值来衡量当前特征对集合划分效果的好坏。
&emsp; ID3决策树算法就是以信息增益为准则划分属性。
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/信息增益计算.jpg)

####信息增益率
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/信息增益率.jpg)
&emsp; C4.5算法就是以增益率为准则来选择最优划分属性。它先从候选划分属性中找出信息增益高于平均水平的属性，然后再从中选择增益率最高的。

####基尼指数
&emsp; CART决策树使用基尼指数来选择划分属性。
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/基尼指数.jpg)

####预剪枝和后剪枝
剪枝操作是为了防止过拟合的手段之一，提升模型的泛化能力。
#####预剪枝
是在构造决策树的过程中，对每个节点在划分前进行估计，若划分不能带来模型泛化能力的提升则不
对当前节点进行划分，并把当前节点标记为叶子节点。
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/预剪枝.jpg)
&emsp; 由于划分后判断凹陷为好瓜，稍凹为好瓜，平坦为坏瓜（这是由于样本中的好坏瓜比例决定的），所以根据验证集中的凹陷->好瓜，稍凹->好瓜，平坦->坏瓜按比例得到精度
#####后剪枝
######错误率降低剪枝（Reduced-Error Pruning，REP）
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/后剪枝1.jpg)
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/后剪枝2.jpg)
####连续值处理
对于周志华机器学习表4.3西瓜数据集的密度属性来说，首先将密度从小到大排列，然后计算划分值，相邻两值相加除以2，然后计算各个t的信息增益。
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/连续值处理.jpg)
再和其他属性的信息增益比较找到最大的属性进行划分，注意密度划分可能出现多次，只是每次的取值范围不同。
###缺失值处理
主要考虑两个问题：
1. 如何在属性值缺失的情况下进行划分属性的选择？
2. 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？
   
对于第一个问题：是将缺失值的地方去掉，算出缺失值占的总值的比例（权重），然后根据去掉缺失值后的数据算出各属性的熵增，然后乘以这个权重。
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/缺失值1.jpg)
比较发现：纹理的信息增益最大，因此使用纹理进行划分。
那么问题来了，8和10在纹理的属性值是缺失的。那么他们应该划分到哪个分支里呢？
解决方法就是将这两个样本同时放入三个分支中，但是给出不同的权重。根据每个分支内数据的个数占总数据的比例。
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/缺失值2.jpg)
接着再递归的构建决策树，从纹理=稍糊开始，样本集为{7,8,9,10,13,14,17}
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/缺失值3.jpg)
具体计算过程如下：
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/缺失值4.jpg)
####标称型数据和数值型数据 
1. 标称型：一般在有限的数据中取，而且只存在‘是’和‘否’两种不同的结果（一般用于分类）
2. 数值型：可以在无限的数据中取，而且数值比较具体化，例如4.02,6.23这种值（一般用于回归分析）

###逻辑回归和线性回归
线性回归可以预测连续值，而无法解决分类问题，而逻辑回归将线性回归的结果，通过某函数映射到(0，1)之间，从而解决分类问题，就是在线性回归上加激活函数

###梯度下降
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/梯度下降1.png)
###梯度下降矩阵描述
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/梯度约定.png)
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/梯度下降4.png)
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/梯度下降3.png)
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/梯度矩阵描述1.png)
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/梯度矩阵描述2.png)
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/梯度矩阵描述3.png)

###正规化线性回归代价函数：
$$J(\theta)=\cfrac{1}{2m}\sum_{i=1}^m[((h_{\theta}(x^{(i)}-y^{(i)}))^2+\lambda\sum_{j=1}^n\theta_j^2)]$$
**梯度下降：**
$$\theta_j:=\theta_j-\alpha[\cfrac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})x_j^{(i)}+\cfrac{\lambda}{m}\theta_j)]$$
**即：**
$$\theta_j:=\theta_j(1-\alpha\cfrac{\lambda}{m}))-\alpha\cfrac{1}{m}[\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})x_j^{(i)}]$$

**注：$\theta_0不参加任何正则化$**

###正规方程解参数
正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$$\cfrac{\alpha}{\alpha\theta_j}J(\theta_j)=0$$
假设我们的训练集特征矩阵为 𝑋（包含了 𝑥0 = 1）并且我们的训练集结果为向量 𝑦，则利用正规方程解出向量:
$$\theta=(X^TX)^{-1}X^Ty$$
例如：
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B.jpg)
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B2.jpg)

###特征缩放
在进行多变量的线性回归之前必须进行特征缩放。
为什么要做特征缩放？若两个单位相差很大的特征，画出等高线会发现图像很扁，梯度下降算法会需要多次迭代。解决方法就是尝试把所有特征的尺度缩放到-1到1之间。最简单的就是：$$x_n=\cfrac{x_n-\mu_n}{s_n} $$
其中：其中 𝜇𝑛是平均值，𝑠𝑛是标准差

###凸函数与凹函数
**与高数学的凸函数，凹函数概念相反**
####凸函数一阶导判断
$$f(y)\ge f(x)+ \nabla_xf(x)^T(y-x)$$
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/凸函数一阶导.jpg)
####凸函数二阶导判断
**二阶导大于等于0就是凸函数(正凸负凹)**
####结构分析法
1. 指数函数是凸函数；
2. 对数函数是凹函数，然后负对数函数就是凸函数；
3. 对于一个凸函数进行仿射变换，可以理解为线性变换，结果还是凸函数；
4. 二次函数是凸函数（二次项系数为正）；
5. 高斯分布函数是凹函数；
6. 多个凸函数的线性加权，如果权值是大于等于零的，那么整个加权结果函数是凸函数。

###逻辑回归假设函数
$$h_\theta(x)=\cfrac{1}{1+e^{-\theta^T x}}$$

###逻辑回归的代价函数
$$ J(\theta)=\cfrac{1}{m} \begin{matrix} \sum_{i=1}^m   \end{matrix} Cost(h_\theta(x^{(i)},y^{(i)}))
 $$
其中：![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/逻辑回归损失函数.jpg)
h(x)与Cost函数关系：
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/hx与代价函数关系.jpg)

####逻辑回归Cost函数特点：
当实际y=1且h(x)=1时，误差为0，当y=1但h(x)不为1时，误差随着h(x)减小而增大。当实际y=0且h(x)=0时，误差为0。当y=0但h(x)不为1时，误差随着h(x)增大而增大
**将Cost函数简化为：**
$$Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))$$
即：
$$J(\theta)=-\cfrac{1}{m}\sum_{i=1}^m[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$$

**梯度：**
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/梯度.jpg)
$$\nabla = \cfrac{1}{m}\sum_{i=1}^m[h_\theta(x^{(i)})-y^{(i)}]x_j^{(i)}$$

###逻辑回归出现RuntimeWarning: divide by zero encountered in log
原因是log函数的指数不能为0，解决方案为：
```
epsilon = 1e-5
J = -(y_batch*np.log(h+epsilon)+(1.0-y_batch)*np.log(1.0-h+epsilon))/m
```

###协方差
是一个反映两个随机变量相关程度的指标，如果一个变量跟随着另一个变量同时变大或者变小，那么这两个变量的协方差就是正值，反之相反，公式如下：
$$
    \begin{aligned}
        Cov(X,Y)=&E[(X-E[X]))(Y-E[Y])]\\
                =&E[XY]-2E[Y]E[X]+E[X]E[Y]\\
                =&E[XY]-E[X]E[Y]
    \end{aligned}
$$

###决定系数
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/决定系数.jpg)

&emsp; 在统计学中用于度量因变量的变异中可由自变量解释部分所占的比例，以此来判断统计模型的解释力。
&emsp; 对于简单线性回归而言，决定系数为样本相关系数的平方。当加入其他回归自变量后，决定系数相应地变为多重相关系数的平方。
&emsp; 假设一数据集包括y1,...,yn共n个观察值，相对应的模型预测值分别为f1,...,fn。定义残差ei = yi − fi，平均观察值为:
$$\bar{y}=\cfrac{1}{n}\begin{matrix}
\sum_{i=1}^ny_i
\end{matrix}$$
于是可以得到总平方和：
$$SS_{tot}=\sum_{i}(y_i-\bar{y})^2
$$
回归平方和：
$$SS_{reg}=\sum_i(f_i-\bar{y})^2$$
残差平方和：
$$SS_{res}=\sum_i(y_i-f_i)^2$$
由此决定系数等于：
$$\begin{aligned}
    R^2=&\cfrac{SS_{reg}}{SS_{tot}}=1-\cfrac{SS_{res}}{SS_{tot}}\\ 
\end{aligned}$$

###过拟合处理
1. 可丢弃一些不能帮助我们正确预测的特征，可手工选择保留哪些特征，或使用某些算法筛选（例如PCA）
2. 正则化。保留所有特征，减少参数的大小

###向量空间
如果把向量看成是空间里的点，那么向量的变换就是点在空间的运动。所以，向量空间是一个集合，这个集合对向量的加法和数乘是封闭的。即只要向量在这个空间内，那么向量按照加法和数乘的方式运动，就会一直在这个空间内


###支持向量机（SVM）
####间隔：
间隔是分类超平面与所有样本距离的最小值，即需要求解使间隔最大化的超平面参数w和b
**拉格朗日乘子法：**
$$L(\omega,b,\alpha)=\cfrac{1}{2}||\omega||^2+\sum_{i=1}^m\alpha_i(1-y_i(\omega^Tx_i+b))$$
$对\omega和b求偏导为零可得：$
$$\omega=\sum_{i=1}^m\alpha_iy_ix_i $$ $$0=\sum_{i=1}^m\alpha_iy_i $$ 


**采用hinge损失：**
$$min(\omega,b) \cfrac{1}{2}||\omega||^2+C\sum_{i=1}^mmax(0,1-y_i(\omega^Tx_i+b))$$

####k分类支持向量机
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/k分类svm.jpg)

####软间隔与松弛变量以及惩罚因子
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/软间隔.jpg)
1. 并非所有的样本点都有一个松弛变量与其对应。实际上只有“离群点”才有，或者也可以这么看，所有没离群的点松弛变量都等于0
2. 松弛变量的值实际上标示出了对应的点到底离群有多远，值越大，点就越远。
3. 惩罚因子C决定了你有多重视离群点带来的损失，显然当所有离群点的松弛变量的和一定时，你定的C越大，对目标函数的损失也越大，此时就暗示着你非常不愿意放弃这些离群点，最极端的情况是你把C定为无限大，这样只要稍有一个点离群，目标函数的值马上变成无限大，马上让问题变成无解，这就退化成了硬间隔问题。
4. 惩罚因子C不是一个变量，整个优化问题在解的时候，C是一个你必须事先指定的值，指定这个值以后，解一下，得到一个分类器，然后用测试数据看看结果怎么样，如果不够好，换一个C的值，再解一次优化问题，得到另一个分类器，再看看效果，如此就是一个参数寻优的过程，但这和优化问题本身决不是一回事，优化问题在解的过程中，C一直是定值，要记住。C越大，表示你越重视，越不想丢弃它们
5. 尽管加了松弛变量这么一说，但这个优化问题仍然是一个优化问题（汗，这不废话么），解它的过程比起原始的硬间隔问题来说，没有任何更加特殊的地方。
6. 松弛变量解决线性不可分的问题，而核函数也是解决线性不可分的问题区别是如果有大量的离群点使用核函数先映射到高维，如果还是不可分则使用松弛变量。
####偏斜问题
也叫数据集偏斜（unbalanced），它指的是参与分类的两个类别（也可以指多个类别）样本数量差异很大。比如说正类有10，000个样本，而负类只给了100个，这会引起的问题显而易见。
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/偏斜.jpg)
&emsp; 方形的点是负类。H，H1，H2是根据给的样本算出来的分类面，由于负类的样本很少很少，所以有一些本来是负类的样本点没有提供，比如图中两个灰色的方形点，如果这两个点有提供的话，那算出来的分类面应该是H’，H2’和H1，他们显然和之前的结果有出入，实际上负类给的样本点越多，就越容易出现在灰色点附近的点，我们算出的结果也就越接近于真实的分类面。但现在由于偏斜的现象存在，使得数量多的正类可以把分类面向负类的方向“推”，因而影响了结果的准确性。
&emsp; 此时可以设置惩罚因子，负类的惩罚因子设置大一些，表示对它重视。
####KKT约束条件
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/kkt1.jpg)
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/kkt2.jpg)

###反向传播算法
####隐含层->输出层权重更新
以权重参数w5为例，如果我们想知道w5对整体误差产生了多少影响，可以用整体误差对w5求偏导求出：（链式法则）
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/反向传播1.jpg)
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/反向传播3.jpg)

####隐含层->隐含层权重更新
方法其实与上面说的差不多，但是有个地方需要变一下，在上文计算总误差对w5的偏导时，是从out(o1)---->net(o1)---->w5,但是在隐含层之间的权值更新时，是out(h1)---->net(h1)---->w1,而out(h1)会接受E(o1)和E(o2)两个地方传来的误差，所以这个地方两个都要计算。
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/反向传播2.jpg)
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/反向传播4.jpg)

**详细推导可见：**
<a href="https://quinwu.github.io/2017/05/16/ML-Neural-Network/">小记,详尽神经网络反向传播推导过程</a>
**推导注意事项：**
$\theta_{i,j}^l$,其中i代表下一层输出的第几个神经元，j代表上一层神经元的第几个神经元，l代表层数

###softmax作为激活函数，交叉熵作为损失函数
交叉熵损失函数：
$$Loss=\sum_iyilna_i$$
y代表真实值，a代表softmax求出的值
由于我们只需要概率最大的数值，因此假设真实值是第j个为1，其余均为0，因此求和符合可以去掉，变成$Loss=-lna_i$

###集成学习
集成中只包含同种类型的个体学习器，此时集成称为“同质”，此时个体学习器称为基学习器。反之，称为“异质”，此时的个体学习器称为组件学习器。
####集成学习分类
1. **Boosting：** 个体学习器之间存在强依赖关系，必须串行生成的序列化方法。
2. **随机森林：** 不存在强依赖关系，可同时并行生成的并行化方法。

####Adaboost（自适应增强）
1. 初始化训练数据的权重。假设有N个样本，则每个样本的初始权重为$\cfrac{1}{N}$
2. 训练弱分类器。若某个样本点被准确分类了，则在构建下一个训练集时，该样本点的权重被降低。相反，未准确分类就会升高它的权重。权重更新后的数据集用于训练下一个分类器。整个训练如此迭代。
3. 组合弱分类器成为强分类器。在各个弱分类器训练结束后，加大分类误差率小的弱分类器权重，使其在最终分类过程中起较大决定作用。同时降低误差率大的弱分类器权重。

**某分类器的误差率等于该分类器被错分样本的权值之和。将训练的弱分类器中误差率最小的作为基本分类器。**

详细例子可见：<a href="https://blog.csdn.net/guyuealian/article/details/70995333">Adaboost详细例子</a>
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/boosting.jpg)

####Bagging
1. 从原始数据集中抽取训练集。每轮从原始数据集中随机抽取n个训练样本，然后放回。（因此，同一个样本可能多次被抽到）。共进行k轮抽取，得到k个训练集。
2. 每次使用一个训练集得到一个模型，k个训练集得到k个模型。（可分类，可回归）
3. 对分类问题，将上面的k个模型采用投票的方式得到分类结果。对于回归问题，计算上述模型的均值作为最后的结果。

Bagging + 决策树 = 随机森林
AdaBoost + 决策树 = 提升树
Gradient Boosting + 决策树 = GBDT

####随机森林
算法过程：
1. 从样本集中通过bootstrap抽样方法（这是一种有放回的抽取），抽取n个样本。
2. 在这n个样本包含的属性中，随机选择一个包含k个属性的属性子集。然后据此按某种策略建立决策树。（这里参数k控制了随机性的引入程度。推荐k=$log_2d$,其中d是当前结点集中属性的个数）
3. 重复m次，建立m棵决策树。
4. 通过bagging的投票机制来进行预测。

###聚类
TF-IDF：词频-逆向文档频率
词频：改词在文档中出现的次数
逆向文档频率：$$log \cfrac{{文档数}}{1+包含单词的文档数}$$
最终使用tf*idf

####k-means（k-均值算法）
样本集D={x~1~,x~2~,....,x~m~}
聚类簇数k
1. 首先从D中随机选择k个样本作为初始均值向量
2. 计算各个样本点到各均值向量的距离。
3. 根据距离最近的均值向量确定该样本的簇标记，即属于哪个簇
4. 根据生成的各个簇，重新计算均值向量（即中心点，更新后的中心点不一定是样本中的点），均值向量计算即是簇中所有向量中对应元素相加除以该向量元素个数。如果新均值和原均值相同，则保持当前均值不变 ，否则将均值更新为计算出的新均值。
5. 重复2，3，4步，直到当前均值向量均未更新。

可以看出k-means其实就是EM算法的体现：E步确定隐含类别变量c，M步更新其他参数u来使得J（即每个样本点到其质心的距离平方和）最小。这里的隐含类别指定方法较特殊，属于硬指定，从k个类别中硬选出一个给样例，而不是对每个类别赋予不同的概率。 

###主成分分析法（PCA）
是一种常见的降维算法。在pca中，需要找到一个方向向量，当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能的小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。
使用基变换来实现降维。降维后的维度就是基向量的个数。使它变成基向量矩阵（行为一个基向量）与要降维矩阵相乘

####协方差：
用于度量两个变量之间的线性相关性程度，若两个变量的协方差为0，则可认为二者线性无关。协方差矩阵则是由变量的协方差值构成的矩阵（对称阵）
$$Cov(X,Y)=\cfrac{\sum_{i=1}^n(X_i-\overline{X})(Y_i-\overline{Y})}{n-1}$$

####特征向量：
矩阵的特征向量是描述数据集结构的非零向量并满足如下公式：
$$A\overrightarrow{v}=\lambda\overrightarrow{v}$$
A是方阵

####协方差矩阵
矩阵中每一行是一个样本，每一列则是一个随机变量（即属性或维度）

![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/协方差1.gif)
![](https://raw.githubusercontent.com/nanfengchuiyeluo6/images/master/协方差2.gif)

####原理 
矩阵的主成分就是其协方差矩阵对应的特征向量，按照对应得特征值大小进行排序，最大的特征值就是第一主成分，其次是第二主成分，以此类推

####算法过程
1. 对所有样本中心化：对某向量先将所有元素加起来然后除以元素个数，再用向量中每个数减去这个数。
2. 计算样本的协方差矩阵$XX^T$
3. 对协方差矩阵做特征值分解
4. 取最大的d个特征值所对应的特征向量

输出投影矩阵
